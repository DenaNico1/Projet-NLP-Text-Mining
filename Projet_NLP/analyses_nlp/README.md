# ğŸ“Š Analyses NLP - MarchÃ© de l'Emploi Data/IA

Analyses complÃ¨tes du corpus de 3024 offres d'emploi Data/IA en France.

---

## ğŸ¯ Vue d'Ensemble

Ce dossier contient **7 analyses NLP** qui transforment vos 3024 offres d'emploi en insights actionnables.

### ğŸ“‹ Les 7 Analyses

| # | Analyse | Objectif | Outputs |
|---|---------|----------|---------|
| **1** | **Preprocessing** | Nettoyage et tokenization | data_preprocessed.pkl |
| **2** | **Extraction CompÃ©tences** | TF-IDF, n-grams, word cloud | competences_extracted.json |
| **3** | **Topic Modeling** | Profils mÃ©tiers (LDA) | topics_lda.json |
| **4** | **GÃ©o-SÃ©mantique** | SpÃ©cificitÃ©s rÃ©gionales | analyse_geo_semantique.json |
| **5** | **Ã‰volution Temporelle** | Tendances compÃ©tences | evolution_temporelle.json |
| **6** | **Embeddings + Clustering** | Vecteurs + visualisation 2D | clustering_2d.html |
| **7** | **Stacks Ã— Salaires** | CorrÃ©lations rÃ©munÃ©ration | stacks_salaires.json |

---

## ğŸš€ Installation

### 1. Installer les DÃ©pendances

```bash
cd analyses_nlp
pip install -r ../requirements.txt
```

**Temps d'installation** : 5-10 minutes

### 2. TÃ©lÃ©charger DonnÃ©es NLTK

```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
```

---

## ğŸ“Š Utilisation

### Option A : Tout Lancer Automatiquement âœ… (RecommandÃ©)

```bash
python run_all_analyses.py
```

**DurÃ©e** : 15-30 minutes  
**RÃ©sultats** : Tous les fichiers dans `../resultats_nlp/`

### Option B : Lancer Analyse par Analyse

```bash
# 1. Preprocessing (obligatoire en premier)
python 1_preprocessing.py

# 2. Extraction compÃ©tences
python 2_extraction_competences.py

# 3. Topic modeling
python 3_topic_modeling.py

# 4. GÃ©o-sÃ©mantique
python 4_geo_semantique.py

# 5. Ã‰volution temporelle
python 5_evolution_temporelle.py

# 6. Embeddings + clustering
python 6_embeddings_clustering.py

# 7. Stacks Ã— salaires
python 7_stacks_salaires.py
```

### Option C : Sauter Certaines Ã‰tapes

```bash
# Sauter preprocessing (dÃ©jÃ  fait) et embeddings (long)
python run_all_analyses.py --skip 1,6
```

---

## ğŸ“ Structure des RÃ©sultats

```
resultats_nlp/
â”œâ”€â”€ ğŸ“„ DonnÃ©es
â”‚   â”œâ”€â”€ data_preprocessed.pkl          # DataFrame nettoyÃ©
â”‚   â”œâ”€â”€ data_with_competences.pkl      # + compÃ©tences extraites
â”‚   â”œâ”€â”€ data_with_topics.pkl           # + topics LDA
â”‚   â””â”€â”€ data_with_clusters.pkl         # + clusters
â”‚
â”œâ”€â”€ ğŸ“Š Analyses JSON
â”‚   â”œâ”€â”€ stats_globales.json            # Statistiques gÃ©nÃ©rales
â”‚   â”œâ”€â”€ competences_extracted.json     # Top compÃ©tences, n-grams
â”‚   â”œâ”€â”€ topics_lda.json                # Profils mÃ©tiers
â”‚   â”œâ”€â”€ analyse_geo_semantique.json    # Par rÃ©gion
â”‚   â”œâ”€â”€ evolution_temporelle.json      # Tendances
â”‚   â”œâ”€â”€ clusters_analysis.json         # Clusters
â”‚   â””â”€â”€ stacks_salaires.json           # CorrÃ©lations
â”‚
â”œâ”€â”€ ğŸ¨ Visualisations
â”‚   â”œâ”€â”€ wordcloud_competences.png
â”‚   â”œâ”€â”€ top30_competences.html
â”‚   â”œâ”€â”€ heatmap_cooccurrence.png
â”‚   â”œâ”€â”€ topics_distribution.html
â”‚   â”œâ”€â”€ carte_regions.html
â”‚   â”œâ”€â”€ clustering_2d.html
â”‚   â”œâ”€â”€ salaires_par_competence.html
â”‚   â””â”€â”€ heatmap_region_competence.html
â”‚
â””â”€â”€ ğŸ§  ModÃ¨les
    â”œâ”€â”€ lda_model.pkl                  # ModÃ¨le LDA entraÃ®nÃ©
    â”œâ”€â”€ embeddings.npy                 # Vecteurs (2000Ã—384)
    â””â”€â”€ umap_coords.npy                # CoordonnÃ©es 2D
```

---

## ğŸ” DÃ©tails des Analyses

### 1ï¸âƒ£ Preprocessing

**EntrÃ©e** : EntrepÃ´t DuckDB (3024 offres)  
**Sortie** : Texte nettoyÃ© + tokenisÃ©

**Ce qui est fait** :
- Nettoyage HTML/URLs/emails
- Tokenization franÃ§ais
- Suppression stopwords
- CrÃ©ation dictionnaire 150+ compÃ©tences

**Fichiers** :
- `data_preprocessed.pkl`
- `dictionnaire_competences.json`
- `stats_globales.json`

---

### 2ï¸âƒ£ Extraction de CompÃ©tences

**MÃ©thodes** :
- Pattern matching (dictionnaire)
- TF-IDF (termes importants)
- N-grams (bi-grams, tri-grams)
- Co-occurrence (paires)

**RÃ©sultats attendus** :
- 100+ compÃ©tences extraites
- Top bi-grams : "machine learning", "deep learning"
- Paires : Python + SQL, Docker + Kubernetes

**Visualisations** :
- Word cloud
- Top 30 bar chart
- Heatmap co-occurrence
- CompÃ©tences par rÃ©gion

---

### 3ï¸âƒ£ Topic Modeling (LDA)

**Algorithme** : Latent Dirichlet Allocation  
**Nombre de topics** : 6

**RÃ©sultats attendus** :
```
Topic 1 (28%): Data Engineering
  â†’ ETL, Spark, Airflow, pipeline

Topic 2 (24%): ML Engineer  
  â†’ model, TensorFlow, deploy, API

Topic 3 (18%): Business Intelligence
  â†’ Power BI, Tableau, dashboard, KPI

Topic 4 (15%): Deep Learning
  â†’ PyTorch, neural network, NLP

Topic 5 (10%): Data Analyst
  â†’ Excel, statistiques, analyse

Topic 6 (5%): MLOps
  â†’ Kubernetes, CI/CD, monitoring
```

**Fichiers** :
- `topics_lda.json`
- `lda_model.pkl`
- `topics_distribution.html`

---

### 4ï¸âƒ£ GÃ©o-SÃ©mantique

**Analyse** : Vocabulaire spÃ©cifique par rÃ©gion

**RÃ©sultats attendus** :
- **Ãle-de-France** : Deep Learning, FinTech, startup
- **Auvergne-RA** : BI, industrie, ERP
- **Occitanie** : AÃ©rospatial, cloud

**Visualisations** :
- Carte interactive France
- Top termes/rÃ©gion
- Salaires/rÃ©gion

---

### 5ï¸âƒ£ Ã‰volution Temporelle

**Analyse** : Tendances compÃ©tences dans le temps

**RÃ©sultats attendus** :
- LangChain : +300% (Nov â†’ DÃ©c)
- MLOps : Croissance stable
- GenAI : Ã‰mergent

**Fichiers** :
- `evolution_temporelle.json`

---

### 6ï¸âƒ£ Embeddings + Clustering

**Algorithme** :
- Sentence-BERT (embeddings)
- UMAP (rÃ©duction 2D)
- K-Means (8 clusters)

**RÃ©sultats attendus** :
- Clustering visuel 2D interactif
- 8 groupes d'offres similaires
- Chaque point = 1 offre (hover = dÃ©tails)

**âš ï¸ Note** : Analyse la plus longue (5-10 min)

**Fichiers** :
- `clustering_2d.html` â† Visualisation interactive !
- `embeddings.npy`
- `clusters_analysis.json`

---

### 7ï¸âƒ£ Stacks Ã— Salaires

**Analyse** : CorrÃ©lations compÃ©tences â†” rÃ©munÃ©ration

**RÃ©sultats attendus** :

```
Top compÃ©tences rÃ©munÃ©rÃ©es :
1. Kubernetes    : 72kâ‚¬
2. MLOps         : 68kâ‚¬
3. PyTorch       : 65kâ‚¬
4. TensorFlow    : 62kâ‚¬
5. Docker        : 58kâ‚¬

Stacks :
- MLOps Stack    : 72kâ‚¬ (87 offres)
- ML Engineer    : 62kâ‚¬ (289 offres)
- Data Analyst   : 42kâ‚¬ (456 offres)
```

**Visualisations** :
- Box plots par compÃ©tence
- Bar chart stacks
- Heatmap rÃ©gion Ã— compÃ©tence

---

## ğŸ› DÃ©pannage

### Erreur "Module not found"

```bash
pip install -r ../requirements.txt
```

### Erreur NLTK

```python
import nltk
nltk.download('all')
```

### Erreur MÃ©moire (Embeddings)

RÃ©duire l'Ã©chantillon dans `6_embeddings_clustering.py` :
```python
# Ligne 30
df_sample = df.sample(min(1000, len(df)), random_state=42)
```

### Analyse Trop Longue

```bash
# Sauter embeddings
python run_all_analyses.py --skip 6
```

---

## ğŸ“Š Utilisation des RÃ©sultats

### Dans Python

```python
import pickle
import json

# Charger donnÃ©es avec compÃ©tences
with open('../resultats_nlp/data_with_topics.pkl', 'rb') as f:
    df = pickle.load(f)

# Charger rÃ©sultats JSON
with open('../resultats_nlp/competences_extracted.json', 'r') as f:
    comps = json.load(f)

print(comps['top_competences'][:10])
```

### Visualisations HTML

Ouvrir directement dans navigateur :
- `clustering_2d.html`
- `top30_competences.html`
- `carte_regions.html`

---

## â±ï¸ Temps d'ExÃ©cution

| Analyse | DurÃ©e |
|---------|-------|
| Preprocessing | 1-2 min |
| Extraction compÃ©tences | 2-3 min |
| Topic modeling | 3-5 min |
| GÃ©o-sÃ©mantique | 1-2 min |
| Ã‰volution temporelle | 1 min |
| Embeddings + clustering | **5-10 min** âš ï¸ |
| Stacks Ã— salaires | 2-3 min |
| **TOTAL** | **15-30 min** |

---

## ğŸ¯ Prochaines Ã‰tapes

AprÃ¨s avoir lancÃ© les analyses :

1. âœ… Consulter les visualisations HTML
2. âœ… Analyser les fichiers JSON
3. âœ… Passer Ã  l'application Streamlit
4. âœ… IntÃ©grer dans le rapport acadÃ©mique

---

## ğŸ“ Support

En cas de problÃ¨me :
1. VÃ©rifier les logs dans la console
2. VÃ©rifier que `entrepot_nlp.duckdb` existe
3. VÃ©rifier les dÃ©pendances installÃ©es

**Tout est prÃªt ! Lancez** :
```bash
python run_all_analyses.py
```

ğŸš€ **Bon courage !**