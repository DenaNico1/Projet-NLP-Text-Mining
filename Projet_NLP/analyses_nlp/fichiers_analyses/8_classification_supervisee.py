"""
Analyse 8 : Classification Supervis√©e des Profils M√©tiers
Entra√Æne des mod√®les (SVM, MLP) pour pr√©dire le profil m√©tier d'une offre

Auteur: Projet NLP Text Mining - Master SISE
Date: D√©cembre 2025
"""

import sys
from pathlib import Path
import pickle
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns

# Ajouter utils au path (g√©rer le fait qu'on est dans fichiers_analyses/)
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))
from utils import ResultSaver

print("=" * 80)
print("ANALYSE 8 : CLASSIFICATION SUPERVIS√âE DES PROFILS M√âTIERS")
print("=" * 80)

# ============================================================================
# 1. CHARGEMENT DES DONN√âES
# ============================================================================

print("\nüì• Chargement des donn√©es...")

# Charger les donn√©es avec topics
data_path = Path(__file__).parent.parent / "resultats_nlp" / "models" / "data_with_topics.pkl"

# Si le fichier n'existe pas, essayer dans le dossier parent
if not data_path.exists():
    data_path = Path(__file__).parent.parent.parent / "resultats_nlp" / "models" / "data_with_topics.pkl"

if not data_path.exists():
    print("‚ùå Erreur : fichier data_with_topics.pkl introuvable")
    print(f"   Cherch√© dans : {data_path}")
    print("üí° Lancez d'abord le script 3_topic_modeling.py")
    sys.exit(1)

with open(data_path, 'rb') as f:
    df = pickle.load(f)

print(f"‚úÖ {len(df)} offres charg√©es")

# V√©rifier qu'on a bien les topics
if 'topic_dominant' not in df.columns:
    print("‚ùå Erreur : colonne 'topic_dominant' manquante")
    print("üí° Lancez d'abord le script 3_topic_modeling.py")
    sys.exit(1)

# ============================================================================
# 2. PR√âPARATION DES DONN√âES
# ============================================================================

print("\nüîß Pr√©paration des donn√©es...")

# Mapper les topics vers des labels textuels
topic_labels = {
    0: "Data Engineering",
    1: "ML Engineering", 
    2: "Business Intelligence",
    3: "Deep Learning",
    4: "Data Analysis",
    5: "MLOps"
}

# Cr√©er la variable cible
df['profil'] = df['topic_dominant'].map(topic_labels)

# Distribution des profils
print("\nüìä Distribution des profils :")
print(df['profil'].value_counts())
print()

# Filtrer les offres avec description
df_clean = df[df['description_clean'].notna()].copy()
print(f"‚úÖ {len(df_clean)} offres avec description")

# ============================================================================
# 3. VECTORISATION TF-IDF
# ============================================================================

print("\nüî§ Vectorisation TF-IDF...")

# On utilise TF-IDF au lieu de BoW pour mieux capturer l'importance
vectorizer = TfidfVectorizer(
    max_features=500,  # Top 500 termes (compromis performance/qualit√©)
    min_df=5,          # Minimum 5 documents
    max_df=0.7,        # Maximum 70% des documents
    token_pattern=r'\b[a-z√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß]{3,}\b'
)

# Fit et transform sur toutes les donn√©es
X = vectorizer.fit_transform(df_clean['description_clean'])

print(f"‚úÖ Matrice : {X.shape[0]} documents √ó {X.shape[1]} features")

# Variable cible
y = df_clean['profil']

# ============================================================================
# 4. D√âCOUPAGE TRAIN / TEST
# ============================================================================

print("\n‚úÇÔ∏è  D√©coupage Train / Test (80/20)...")

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y  # Garder les m√™mes proportions
)

print(f"‚úÖ Train : {X_train.shape[0]} offres")
print(f"‚úÖ Test  : {X_test.shape[0]} offres")

# V√©rifier stratification
print("\nüìä Distribution Train :")
print(y_train.value_counts(normalize=True).round(3))
print("\nüìä Distribution Test :")
print(y_test.value_counts(normalize=True).round(3))

# ============================================================================
# 5. MOD√àLE 1 : SVM avec GridSearchCV
# ============================================================================

print("\n" + "=" * 80)
print("MOD√àLE 1 : SVM (Support Vector Machine)")
print("=" * 80)

print("\nüîç Recherche des meilleurs hyperparam√®tres (GridSearchCV)...")

# Grille de param√®tres √† tester
param_grid_svm = {
    'kernel': ['linear', 'rbf'],
    'C': [0.1, 0.5, 1.0, 2.0, 10.0]
}

# Instance SVM
svm = SVC(random_state=42)

# GridSearchCV avec validation crois√©e 5-fold
grid_svm = GridSearchCV(
    estimator=svm,
    param_grid=param_grid_svm,
    scoring='f1_weighted',  # F1 pond√©r√© pour classes d√©s√©quilibr√©es
    cv=5,
    verbose=1,
    n_jobs=-1
)

# Entra√Ænement (peut prendre quelques minutes)
print("‚è≥ Entra√Ænement en cours...")
grid_svm.fit(X_train, y_train)

# Meilleurs param√®tres
print(f"\n‚úÖ Meilleurs param√®tres : {grid_svm.best_params_}")
print(f"‚úÖ F1-score (CV) : {grid_svm.best_score_:.3f}")

# √âvaluation sur le test
y_pred_svm = grid_svm.predict(X_test)

print("\nüìä R√âSULTATS SVM SUR TEST :")
print(f"Accuracy : {accuracy_score(y_test, y_pred_svm):.3f}")
print("\nClassification Report :")
print(classification_report(y_test, y_pred_svm))

# Matrice de confusion
cm_svm = confusion_matrix(y_test, y_pred_svm, labels=grid_svm.classes_)

# ============================================================================
# 6. MOD√àLE 2 : Perceptron Multi-Couches (MLP)
# ============================================================================

print("\n" + "=" * 80)
print("MOD√àLE 2 : PERCEPTRON MULTI-COUCHES (MLP)")
print("=" * 80)

print("\nüîç Recherche des meilleurs hyperparam√®tres (GridSearchCV)...")

# Grille de param√®tres
param_grid_mlp = {
    'hidden_layer_sizes': [(50,), (100,), (50, 25)],
    'activation': ['tanh', 'relu'],
    'alpha': [0.0001, 0.001, 0.01]
}

# Instance MLP
mlp = MLPClassifier(random_state=42, max_iter=500)

# GridSearchCV
grid_mlp = GridSearchCV(
    estimator=mlp,
    param_grid=param_grid_mlp,
    scoring='f1_weighted',
    cv=5,
    verbose=1,
    n_jobs=-1
)

# Entra√Ænement
print("‚è≥ Entra√Ænement en cours...")
grid_mlp.fit(X_train, y_train)

# Meilleurs param√®tres
print(f"\n‚úÖ Meilleurs param√®tres : {grid_mlp.best_params_}")
print(f"‚úÖ F1-score (CV) : {grid_mlp.best_score_:.3f}")

# √âvaluation sur le test
y_pred_mlp = grid_mlp.predict(X_test)

print("\nüìä R√âSULTATS MLP SUR TEST :")
print(f"Accuracy : {accuracy_score(y_test, y_pred_mlp):.3f}")
print("\nClassification Report :")
print(classification_report(y_test, y_pred_mlp))

# Matrice de confusion
cm_mlp = confusion_matrix(y_test, y_pred_mlp, labels=grid_mlp.classes_)

# ============================================================================
# 7. COMPARAISON DES MOD√àLES
# ============================================================================

print("\n" + "=" * 80)
print("COMPARAISON DES MOD√àLES")
print("=" * 80)

# Scores
scores = {
    'SVM': accuracy_score(y_test, y_pred_svm),
    'MLP': accuracy_score(y_test, y_pred_mlp)
}

print("\nüìä Accuracy sur Test :")
for model, score in scores.items():
    print(f"  {model} : {score:.3f}")

# Meilleur mod√®le
best_model_name = max(scores, key=scores.get)
best_model = grid_svm if best_model_name == 'SVM' else grid_mlp

print(f"\nüèÜ Meilleur mod√®le : {best_model_name} ({scores[best_model_name]:.3f})")

# ============================================================================
# 8. VISUALISATIONS
# ============================================================================

print("\nüìä Cr√©ation des visualisations...")

# Cr√©er dossier visualisations
viz_dir = Path(__file__).parent.parent / "resultats_nlp" / "visualisations"
viz_dir.mkdir(parents=True, exist_ok=True)  # parents=True cr√©e aussi resultats_nlp si besoin

# 8.1 Matrices de confusion
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# SVM
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=grid_svm.classes_, 
            yticklabels=grid_svm.classes_,
            ax=axes[0])
axes[0].set_title('Matrice de Confusion - SVM')
axes[0].set_xlabel('Pr√©dit')
axes[0].set_ylabel('R√©el')

# MLP
sns.heatmap(cm_mlp, annot=True, fmt='d', cmap='Greens',
            xticklabels=grid_mlp.classes_,
            yticklabels=grid_mlp.classes_,
            ax=axes[1])
axes[1].set_title('Matrice de Confusion - MLP')
axes[1].set_xlabel('Pr√©dit')
axes[1].set_ylabel('R√©el')

plt.tight_layout()
plt.savefig(viz_dir / 'confusion_matrices.png', dpi=150, bbox_inches='tight')
plt.close()

print(f"‚úÖ Matrices de confusion : {viz_dir / 'confusion_matrices.png'}")

# 8.2 Comparaison des scores par classe
from sklearn.metrics import f1_score

classes = grid_svm.classes_

scores_by_class = {
    'SVM': [f1_score(y_test, y_pred_svm, labels=[c], average='weighted') 
            for c in classes],
    'MLP': [f1_score(y_test, y_pred_mlp, labels=[c], average='weighted') 
            for c in classes]
}

df_scores = pd.DataFrame(scores_by_class, index=classes)

fig, ax = plt.subplots(figsize=(12, 6))
df_scores.plot(kind='bar', ax=ax, width=0.8)
ax.set_title('F1-Score par Profil M√©tier')
ax.set_xlabel('Profil')
ax.set_ylabel('F1-Score')
ax.set_ylim(0, 1)
ax.legend(['SVM', 'MLP'])
ax.grid(axis='y', alpha=0.3)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig(viz_dir / 'f1_scores_by_profile.png', dpi=150, bbox_inches='tight')
plt.close()

print(f"‚úÖ F1-scores par profil : {viz_dir / 'f1_scores_by_profile.png'}")

# ============================================================================
# 9. SAUVEGARDE DES R√âSULTATS
# ============================================================================

print("\nüíæ Sauvegarde des r√©sultats...")

saver = ResultSaver()

# Sauvegarder les mod√®les
saver.save_pickle(grid_svm, 'model_svm.pkl')
saver.save_pickle(grid_mlp, 'model_mlp.pkl')
saver.save_pickle(vectorizer, 'vectorizer_classification.pkl')

print(f"‚úÖ Mod√®les sauvegard√©s dans resultats_nlp/models/")

# R√©sultats JSON
results = {
    'svm': {
        'best_params': grid_svm.best_params_,
        'cv_f1_score': float(grid_svm.best_score_),
        'test_accuracy': float(accuracy_score(y_test, y_pred_svm)),
        'classification_report': classification_report(y_test, y_pred_svm, output_dict=True)
    },
    'mlp': {
        'best_params': grid_mlp.best_params_,
        'cv_f1_score': float(grid_mlp.best_score_),
        'test_accuracy': float(accuracy_score(y_test, y_pred_mlp)),
        'classification_report': classification_report(y_test, y_pred_mlp, output_dict=True)
    },
    'best_model': best_model_name,
    'topic_labels': topic_labels
}

saver.save_json(results, 'classification_results.json')

print(f"‚úÖ R√©sultats sauvegard√©s : resultats_nlp/classification_results.json")

# ============================================================================
# 10. R√âCAPITULATIF
# ============================================================================

print("\n" + "=" * 80)
print("‚úÖ CLASSIFICATION SUPERVIS√âE TERMIN√âE")
print("=" * 80)

print(f"""
üìä R√©sultats finaux :

SVM :
  - Accuracy Test : {scores['SVM']:.3f}
  - F1-Score CV   : {grid_svm.best_score_:.3f}
  - Params        : {grid_svm.best_params_}

MLP :
  - Accuracy Test : {scores['MLP']:.3f}
  - F1-Score CV   : {grid_mlp.best_score_:.3f}
  - Params        : {grid_mlp.best_params_}

üèÜ Meilleur mod√®le : {best_model_name}

üìÅ Fichiers cr√©√©s :
  - models/model_svm.pkl
  - models/model_mlp.pkl
  - models/vectorizer_classification.pkl
  - classification_results.json
  - visualisations/confusion_matrices.png
  - visualisations/f1_scores_by_profile.png
""")

print("‚ú® Les mod√®les peuvent maintenant √™tre utilis√©s dans l'application Streamlit !")