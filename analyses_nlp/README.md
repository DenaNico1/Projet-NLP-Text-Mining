# üî¨ ANALYSES NLP - Data IA Talent Observatory

**Pipeline complet de Text Mining pour l'analyse du march√© Data/IA**

---

## üìã VUE D'ENSEMBLE

Ce dossier contient l'ensemble du **pipeline NLP/ML** pour l'analyse des offres d'emploi Data/IA en France. Il impl√©mente **9 analyses scientifiques** allant du preprocessing √† la classification supervis√©e, en passant par le topic modeling et l'extraction de comp√©tences.

### üéØ Objectifs

- ‚úÖ Pr√©traiter 3,023 descriptions d'offres (tokenization, nettoyage)
- ‚úÖ Extraire automatiquement 770 comp√©tences techniques
- ‚úÖ D√©couvrir 6 profils m√©tiers via Topic Modeling (LDA)
- ‚úÖ Classifier offres avec 90% de pr√©cision (SVM)
- ‚úÖ Identifier comp√©tences "signature" par profil (Chi¬≤)
- ‚úÖ Analyser sp√©cificit√©s g√©ographiques et temporelles

---

## üóÇÔ∏è STRUCTURE DOSSIER

```
analyses_nlp/
‚îÇ
‚îú‚îÄ‚îÄ README.md                               # Ce fichier
‚îú‚îÄ‚îÄ DOCUMENTATION_ANALYSES_NLP.md           # Doc technique compl√®te
‚îÇ
‚îú‚îÄ‚îÄ 0_preparation_donnees.py                # Chargement donn√©es entrep√¥t
‚îú‚îÄ‚îÄ 1_preprocessing.py                      # Nettoyage, tokenization (NLTK)
‚îú‚îÄ‚îÄ 2_extraction_competences.py             # Extraction 770 comp√©tences
‚îú‚îÄ‚îÄ 3_topic_modeling.py                     # LDA (k=6, coherence=0.78)
‚îú‚îÄ‚îÄ 4_analyse_geo_semantique.py             # Sp√©cificit√©s r√©gionales
‚îú‚îÄ‚îÄ 5_analyse_temporelle.py                 # √âvolution tendances
‚îú‚îÄ‚îÄ 6_clustering.py                         # UMAP + K-Means
‚îú‚îÄ‚îÄ 7_analyse_stacks_salaires.py            # Correlation comp√©tences-salaires
‚îú‚îÄ‚îÄ 8_classification_supervisee.py          # SVM (89.6%), MLP (89.4%)
‚îú‚îÄ‚îÄ 9_selection_features_chi2.py            # Comp√©tences discriminantes
‚îÇ
‚îú‚îÄ‚îÄ hybrid_classification.py                # Syst√®me hybride 3 couches
‚îú‚îÄ‚îÄ apply_hybrid_classification.py          # Script application hybride
‚îÇ
‚îú‚îÄ‚îÄ dictionnaire_competences.json           # 770 comp√©tences + patterns
‚îú‚îÄ‚îÄ stopwords_custom.txt                    # Stopwords domaine Data/IA
‚îÇ
‚îî‚îÄ‚îÄ resultats_nlp/
    ‚îú‚îÄ‚îÄ models/
    ‚îÇ   ‚îú‚îÄ‚îÄ lda_model.pkl                   # Mod√®le LDA (fig√© v1)
    ‚îÇ   ‚îú‚îÄ‚îÄ lda_vectorizer.pkl              # CountVectorizer
    ‚îÇ   ‚îú‚îÄ‚îÄ model_svm.pkl                   # SVM classifieur
    ‚îÇ   ‚îú‚îÄ‚îÄ model_mlp.pkl                   # MLP classifieur
    ‚îÇ   ‚îú‚îÄ‚îÄ vectorizer_classification.pkl   # TF-IDF
    ‚îÇ   ‚îú‚îÄ‚îÄ umap_model.pkl                  # UMAP embeddings
    ‚îÇ   ‚îú‚îÄ‚îÄ kmeans_model.pkl                # K-Means clusters
    ‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessed.pkl           # Donn√©es pr√©trait√©es
    ‚îÇ   ‚îú‚îÄ‚îÄ data_with_topics.pkl            # Donn√©es + topics LDA
    ‚îÇ   ‚îî‚îÄ‚îÄ data_with_hybrid_profiles.pkl   # Donn√©es + profils hybrides
    ‚îÇ
    ‚îú‚îÄ‚îÄ visualizations/
    ‚îÇ   ‚îú‚îÄ‚îÄ wordclouds/                     # Nuages de mots par profil
    ‚îÇ   ‚îú‚îÄ‚îÄ topic_distribution.png          # Distribution topics
    ‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix_svm.png        # Matrice confusion
    ‚îÇ   ‚îú‚îÄ‚îÄ umap_projection.png             # Projection UMAP
    ‚îÇ   ‚îî‚îÄ‚îÄ correlation_competences.png     # Heatmap comp√©tences
    ‚îÇ
    ‚îú‚îÄ‚îÄ lda_topics.json                     # Topics + top terms
    ‚îú‚îÄ‚îÄ chi2_selection.json                 # Features Chi¬≤ par profil
    ‚îú‚îÄ‚îÄ classification_results.json         # M√©triques SVM/MLP
    ‚îú‚îÄ‚îÄ geo_analysis.json                   # Sp√©cificit√©s r√©gionales
    ‚îú‚îÄ‚îÄ temporal_analysis.json              # Tendances temporelles
    ‚îú‚îÄ‚îÄ cluster_results.json                # R√©sultats clustering
    ‚îú‚îÄ‚îÄ hybrid_classification_stats.json    # Stats syst√®me hybride
    ‚îî‚îÄ‚îÄ hybrid_classifier_config_v1.json    # Config hybride v1
```

---

## üöÄ GUIDE D'UTILISATION

### **Installation D√©pendances**

```bash
pip install -r requirements.txt
```

**Principales librairies** :
```
pandas>=2.1.0
numpy>=1.26.0
scikit-learn>=1.4.0
nltk>=3.8.1
spacy>=3.7.0
gensim>=4.3.0
umap-learn>=0.5.5
plotly>=5.18.0
seaborn>=0.13.0
```

**T√©l√©chargement ressources NLTK** :
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```

---

### **Ex√©cution Pipeline Complet**

#### **Option 1 : Pipeline s√©quentiel (recommand√© pour 1√®re fois)**

```bash
# √âtape 0 : Pr√©parer donn√©es depuis entrep√¥t
python 0_preparation_donnees.py

# √âtape 1 : Preprocessing (15 min)
python 1_preprocessing.py

# √âtape 2 : Extraction comp√©tences (10 min)
python 2_extraction_competences.py

# √âtape 3 : Topic Modeling LDA (20 min)
python 3_topic_modeling.py --n_topics 6

# √âtape 4 : Analyse g√©o-s√©mantique (5 min)
python 4_analyse_geo_semantique.py

# √âtape 5 : Analyse temporelle (5 min)
python 5_analyse_temporelle.py

# √âtape 6 : Clustering (10 min)
python 6_clustering.py --n_clusters 8

# √âtape 7 : Analyse stacks √ó salaires (5 min)
python 7_analyse_stacks_salaires.py

# √âtape 8 : Classification supervis√©e (15 min)
python 8_classification_supervisee.py

# √âtape 9 : S√©lection features Chi¬≤ (5 min)
python 9_selection_features_chi2.py

# √âtape 10 : Classification hybride (5 min)
python apply_hybrid_classification.py
```

**Dur√©e totale** : ~1h30

---

#### **Option 2 : Pipeline automatis√©**

```bash
# Script master qui ex√©cute tout
python run_full_pipeline.py
```

---

### **Ex√©cution Scripts Individuels**

#### **Script 1 : Preprocessing**

```bash
python 1_preprocessing.py
```

**Input** : `resultats_nlp/models/data_raw.pkl` (depuis entrep√¥t)  
**Output** : `resultats_nlp/models/data_preprocessed.pkl`

**Ce qui est fait** :
- Tokenization (NLTK)
- Lowercasing
- Suppression stopwords (fran√ßais + custom)
- Suppression ponctuation
- Conservation tokens alphanum√©riques

**Param√®tres modifiables** :
```python
# Dans 1_preprocessing.py
MIN_TOKEN_LENGTH = 2      # Longueur minimale token
STOPWORDS_CUSTOM = [...]  # Stopwords domaine
```

---

#### **Script 2 : Extraction Comp√©tences**

```bash
python 2_extraction_competences.py
```

**Input** : 
- `data_preprocessed.pkl`
- `dictionnaire_competences.json` (770 comp√©tences)

**Output** : 
- `data_preprocessed.pkl` (enrichi avec colonne `competences_found`)

**M√©thode** :
- Pattern matching regex (case-insensitive)
- Validation contexte (‚â•2 caract√®res, pas dans stopwords)
- Fr√©quence par offre

**Top 10 comp√©tences d√©tect√©es** :
```
1. Python       : 2,145 offres (71%)
2. SQL          : 1,987 offres (66%)
3. Machine Learning : 1,456 offres (48%)
4. Pandas       : 1,234 offres (41%)
5. Spark        : 987 offres (33%)
...
```

**Ajouter nouvelle comp√©tence** :
```json
// Dans dictionnaire_competences.json
{
  "langages": {
    "Rust": {
      "patterns": ["\\brust\\b"],
      "categorie": "Langage"
    }
  }
}
```

---

#### **Script 3 : Topic Modeling (LDA)**

```bash
python 3_topic_modeling.py --n_topics 6 --max_iter 1000
```

**Hyperparam√®tres** :
```bash
--n_topics        # Nombre de topics (d√©faut: 6)
--alpha           # Prior Dirichlet docs-topics (d√©faut: 0.1)
--beta            # Prior Dirichlet topics-mots (d√©faut: 0.01)
--max_iter        # Nombre it√©rations (d√©faut: 1000)
--random_state    # Seed (d√©faut: 42)
```

**Output** :
- `models/lda_model.pkl` (mod√®le scikit-learn)
- `models/lda_vectorizer.pkl` (CountVectorizer)
- `lda_topics.json` (topics + top 20 termes)
- `data_with_topics.pkl` (donn√©es + colonne `topic_dominant`)

**√âvaluation** :
```python
# Coherence score (plus √©lev√© = meilleur)
Coherence : 0.78  # Excellent (>0.7)

# Perplexity (plus bas = meilleur)
Perplexity : -8.2  # Bon (<-7)
```

**Topics d√©couverts** :
```
Topic 0 - Data Engineering (24%)
  spark, airflow, sql, etl, kafka, hive, hadoop, python

Topic 1 - ML Engineering (16%)
  machine, learning, scikit, model, python, pandas, tensorflow

Topic 2 - Business Intelligence (13%)
  power, bi, tableau, qlik, dax, sql, excel, reporting

Topic 3 - Deep Learning (24%)
  deep, learning, pytorch, tensorflow, neural, network, cnn

Topic 4 - Data Analysis (7%)
  sql, excel, python, pandas, statistics, analysis

Topic 5 - MLOps (28%)
  kubernetes, docker, mlops, ci, cd, terraform, jenkins
```

**Tester diff√©rents k** :
```bash
for k in 4 6 8 10; do
    python 3_topic_modeling.py --n_topics $k
done
# Comparer coherence scores
```

---

#### **Script 8 : Classification Supervis√©e**

```bash
python 8_classification_supervisee.py --model svm
```

**Options** :
```bash
--model        # svm | mlp | random_forest | gradient_boosting
--cv_folds     # Nombre folds cross-validation (d√©faut: 5)
--test_size    # Taille test set (d√©faut: 0.2)
```

**Pipeline** :
1. Split train/test (80/20 stratifi√©)
2. Vectorisation TF-IDF (max_features=500)
3. GridSearchCV sur hyperparam√®tres
4. Entra√Ænement meilleur mod√®le
5. √âvaluation (accuracy, precision, recall, F1)
6. Sauvegarde mod√®le

**R√©sultats** :

| Mod√®le | Accuracy | F1 (weighted) | Temps entra√Ænement |
|--------|----------|---------------|---------------------|
| **SVM** | 89.6% | 0.896 | 45s |
| MLP | 89.4% | 0.895 | 120s |
| Random Forest | 87.2% | 0.871 | 30s |
| Gradient Boosting | 88.1% | 0.880 | 90s |

**Meilleur mod√®le** : SVM (`kernel='rbf', C=2.0`)

**Matrice confusion** : `visualizations/confusion_matrix_svm.png`

---

#### **Script 9 : Chi¬≤ Selection**

```bash
python 9_selection_features_chi2.py --top_k 100
```

**Objectif** : Identifier comp√©tences "signature" par profil

**M√©thode** :
1. Cr√©er matrice binaire (3,023 √ó 770) : 1 si comp√©tence pr√©sente
2. Chi¬≤ test pour chaque (comp√©tence, profil)
3. S√©lectionner top k features par œá¬≤ score
4. Calculer lift : `P(comp|profil) / P(comp|global)`

**Output** : `chi2_selection.json`
```json
{
  "signature_by_profile": {
    "MLOps": [
      {"competence": "Kubernetes", "chi2": 698.5, "lift": 2.3},
      {"competence": "Docker", "chi2": 645.2, "lift": 2.1},
      ...
    ]
  }
}
```

**Top signatures** :

| Profil | Top 3 Comp√©tences (lift > 1.5) |
|--------|--------------------------------|
| MLOps | Kubernetes (2.3x), Docker (2.1x), Terraform (1.9x) |
| Deep Learning | PyTorch (2.8x), TensorFlow (2.4x), GPU (2.2x) |
| BI | Power BI (3.1x), Tableau (2.7x), Qlik (2.3x) |
| Data Engineering | Spark (2.1x), Airflow (1.9x), Kafka (1.8x) |

**Application** : Gap analysis dans Audit de Profil (Streamlit)

---

#### **Syst√®me Hybride 3 Couches**

```bash
python apply_hybrid_classification.py
```

**Ce que fait le script** :
1. ‚úÖ Charge donn√©es (`data_with_topics.pkl`)
2. ‚úÖ Applique classification 3 couches (titre ‚Üí comp√©tences ‚Üí LDA)
3. ‚úÖ G√©n√®re stats d√©taill√©es (par m√©thode, profil, confiance)
4. ‚úÖ D√©tecte profils √©mergents (titres fr√©quents en fallback)
5. ‚úÖ Sauvegarde r√©sultats (`data_with_hybrid_profiles.pkl`)

**Output console** :
```
üìä STATISTIQUES DE CLASSIFICATION
============================================================
Total offres : 3023

Par m√©thode :
  ‚Ä¢ titre              : 2116 ( 70.0%)
  ‚Ä¢ competences        :  484 ( 16.0%)
  ‚Ä¢ lda_fallback       :  423 ( 14.0%)

Par profil (Top 10) :
  ‚Ä¢ Data Engineer               :  520 ( 17.2%)
  ‚Ä¢ Data Scientist              :  470 ( 15.5%)
  ‚Ä¢ ML Engineer                 :  380 ( 12.6%)
  ‚Ä¢ MLOps Engineer              :  350 ( 11.6%)
  ...

üîç D√âTECTION PROFILS √âMERGENTS
============================================================
Offres en fallback : 423 (14.0%)

Titres fr√©quents non class√©s :
  (aucun si < 10 occurrences)
```

**Ajouter nouveau profil** :
```bash
# 1. √âditer config
nano hybrid_classifier_config_v1.json

# 2. Ajouter pattern
{
  "Prompt Engineer": [
    "prompt engineer",
    "prompt.*engineer"
  ]
}

# 3. Reclassifier
python apply_hybrid_classification.py
```

---

## üìä R√âSULTATS CL√âS

### **M√©triques Globales**

| M√©trique | Valeur | D√©tail |
|----------|--------|--------|
| **Corpus** | 3,023 offres | France Travail (83%) + Indeed (17%) |
| **Vocabulaire** | 12,453 tokens | Apr√®s preprocessing |
| **Comp√©tences uniques** | 770 | 6 cat√©gories |
| **Comp√©tences/offre** | 12.4 (m√©diane) | Min: 0, Max: 45 |
| **Topics LDA** | 6 | Coherence: 0.78 |
| **Profils hybrides** | 14 | Data Scientist, ML Engineer... |
| **Pr√©cision SVM** | 89.6% | F1: 0.896 |
| **Pr√©cision hybride** | 88.7% | Pond√©r√©e par m√©thode |

---

### **Top 10 Comp√©tences**

| Rang | Comp√©tence | Nb Offres | % Corpus |
|------|------------|-----------|----------|
| 1 | Python | 2,145 | 71% |
| 2 | SQL | 1,987 | 66% |
| 3 | Machine Learning | 1,456 | 48% |
| 4 | Pandas | 1,234 | 41% |
| 5 | Spark | 987 | 33% |
| 6 | Docker | 856 | 28% |
| 7 | AWS | 745 | 25% |
| 8 | TensorFlow | 612 | 20% |
| 9 | Kubernetes | 598 | 20% |
| 10 | Tableau | 534 | 18% |

---

### **Distribution Profils Hybrides**

| Profil | Nb Offres | % |
|--------|-----------|---|
| Data Engineering | 520 | 17.2% |
| Data Scientist | 470 | 15.5% |
| ML Engineering | 380 | 12.6% |
| MLOps Engineer | 350 | 11.6% |
| Deep Learning | 280 | 9.3% |
| Data Analyst | 210 | 6.9% |
| BI Analyst | 190 | 6.3% |
| NLP Engineer | 80 | 2.6% |
| **Autres** | 543 | 18.0% |

---

## üîß CONFIGURATION

### **Fichiers Configuration**

#### **dictionnaire_competences.json**

Structure :
```json
{
  "langages": {
    "Python": {
      "patterns": ["\\bpython\\b"],
      "categorie": "Langage",
      "type": "Technique"
    }
  },
  "frameworks_ml": {
    "TensorFlow": {
      "patterns": ["tensorflow", "tf\\."],
      "categorie": "Framework ML",
      "type": "Technique"
    }
  }
}
```

**Cat√©gories** :
- `langages` (45 comp√©tences)
- `frameworks_ml` (120)
- `outils_data` (180)
- `cloud_infra` (95)
- `bi_viz` (65)
- `soft_skills` (265)

---

#### **stopwords_custom.txt**

```
data
ia
intelligence
artificielle
recherche
poste
offre
emploi
candidat
profil
experience
annee
...
```

**Usage** :
```python
# Dans 1_preprocessing.py
custom_stopwords = set(open('stopwords_custom.txt').read().split())
```

---

#### **hybrid_classifier_config_v1.json**

```json
{
  "version": "1.0",
  "date": "2024-12-27",
  "regex_profils": {
    "Data Scientist": [
      "data scientist",
      "scientifique.*donn√©es"
    ]
  },
  "signatures_competences": {
    "Data Scientist": {
      "must_have": ["python", "machine learning"],
      "strong_indicators": ["pandas", "scikit-learn"],
      "threshold": 0.3
    }
  },
  "topic_to_profil": {
    "0": "Data Engineering",
    "1": "ML Engineering",
    ...
  }
}
```

---

## üìà VISUALISATIONS

Toutes les visualisations sont sauvegard√©es dans `resultats_nlp/visualizations/`

### **Disponibles** :

1. **Nuages de mots** (`wordclouds/`)
   - 1 par profil (6 topics LDA)
   - Top 50 termes pond√©r√©s

2. **Distribution topics** (`topic_distribution.png`)
   - Bar chart % corpus par topic

3. **Matrice confusion** (`confusion_matrix_svm.png`)
   - Heatmap 6√ó6 (pr√©cision par classe)

4. **Projection UMAP** (`umap_projection.png`)
   - Scatter 2D color√© par profil
   - Visualise s√©parabilit√©

5. **Corr√©lation comp√©tences** (`correlation_competences.png`)
   - Heatmap co-occurrences top 30 comp√©tences

---

## üêõ TROUBLESHOOTING

### **Erreur : `ModuleNotFoundError: No module named 'nltk'`**

```bash
pip install nltk
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

---

### **Erreur : `FileNotFoundError: data_preprocessed.pkl`**

‚Üí Ex√©cutez d'abord les scripts dans l'ordre (0 ‚Üí 1 ‚Üí 2...)

```bash
python 0_preparation_donnees.py
python 1_preprocessing.py
```

---

### **Erreur : `MemoryError` pendant LDA**

‚Üí R√©duire `max_features` dans CountVectorizer

```python
# Dans 3_topic_modeling.py
vectorizer = CountVectorizer(
    max_features=500,  # Au lieu de 1000
    ...
)
```

---

### **Warning : `Coherence score very low (<0.5)`**

‚Üí Tester diff√©rents `n_topics` :

```bash
for k in 4 6 8 10 12; do
    python 3_topic_modeling.py --n_topics $k
done
```

‚Üí V√©rifier qualit√© preprocessing (trop de stopwords ?)

---

### **Classification hybride : trop d'offres en fallback (>25%)**

‚Üí Ajouter r√®gles Couche 1 (titre) ou Couche 2 (comp√©tences)

```bash
# 1. Analyser titres fr√©quents
python apply_hybrid_classification.py

# 2. √âditer config
nano hybrid_classifier_config_v1.json

# 3. Reclassifier
python apply_hybrid_classification.py
```

---

## üìö DOCUMENTATION COMPL√âMENTAIRE

- üìÑ **DOCUMENTATION_ANALYSES_NLP.md** : Documentation technique compl√®te
- üìÑ **hybrid_classification.py** : Code comment√© syst√®me hybride
- üìÑ **../entrepot_de_donnees/README.md** : Documentation entrep√¥t
- üìÑ **../app_streamlit/README_DATATALENT_OBSERVATORY.md** : Documentation app

---

## üß™ TESTS

### **Validation Pipeline**

```bash
# Test complet sur 100 offres √©chantillon
python test_pipeline.py --sample_size 100

# Output attendu :
# ‚úÖ Preprocessing : 100/100 offres
# ‚úÖ Extraction comp√©tences : 97/100 (‚â•1 comp√©tence)
# ‚úÖ Topic modeling : Coherence > 0.6
# ‚úÖ Classification : Accuracy > 85%
```

---

### **Validation Manuelle**

1. **V√©rifier comp√©tences extraites** :
```python
import pickle
df = pickle.load(open('resultats_nlp/models/data_preprocessed.pkl', 'rb'))
print(df[['titre', 'competences_found']].head(10))
```

2. **V√©rifier topics LDA** :
```python
import json
topics = json.load(open('resultats_nlp/lda_topics.json'))
for topic_id, data in topics.items():
    print(f"Topic {topic_id}: {', '.join(data['top_terms'][:10])}")
```

3. **V√©rifier profils hybrides** :
```python
df = pickle.load(open('resultats_nlp/models/data_with_hybrid_profiles.pkl', 'rb'))
print(df['profil'].value_counts())
print(df['methode'].value_counts())
```

---

## üöÄ OPTIMISATIONS

### **Pour corpus >10k offres**

1. **Preprocessing** : Parall√©lisation
```python
from multiprocessing import Pool

def preprocess_batch(batch):
    # ...
    return batch

with Pool(8) as p:
    results = p.map(preprocess_batch, df_chunks)
```

2. **LDA** : Utiliser Gensim (plus rapide)
```python
from gensim.models import LdaModel
# Au lieu de scikit-learn
```

3. **Classification** : Mini-batch learning
```python
from sklearn.linear_model import SGDClassifier
# Au lieu de SVM complet
```

---

## üìä EXPORTS

### **Exporter r√©sultats vers CSV**

```bash
python export_results.py
```

**Fichiers g√©n√©r√©s** :
- `resultats_nlp/exports/offres_with_profiles.csv` (toutes colonnes)
- `resultats_nlp/exports/competences_frequency.csv` (top 100)
- `resultats_nlp/exports/topics_distribution.csv` (6 topics)
- `resultats_nlp/exports/classification_report.csv` (m√©triques)

---

## üë• CONTRIBUTEURS

**Projet Master SISE - NLP Text Mining**  
D√©cembre 2025

---

## üìÑ LICENCE

Projet acad√©mique - Master SISE

---

## üìû SUPPORT

Pour toute question :
- üìß Email : [votre email]
- üìÇ Repo : [votre repo GitHub]

---

**üî¨ DataTalent Observatory - Pipeline NLP Complet**