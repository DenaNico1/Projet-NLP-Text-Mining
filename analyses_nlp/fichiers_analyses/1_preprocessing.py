"""
PREPROCESSING MASTER - Unique Point de Traitement
Fait TOUT le preprocessing et gÃ©nÃ¨re data_clean.pkl avec toutes les colonnes nÃ©cessaires

VERSION FINALE :
- Stopwords FR+EN complets
- Lemmatisation activÃ©e
- Nettoyage HTML complet
- Pattern matching compÃ©tences
- GÃ©nÃ¨re TOUTES les colonnes pour analyses

Auteur: Projet NLP Text Mining
Date: DÃ©cembre 2025
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import json

# Ajouter le chemin pour imports
sys.path.insert(0, str(Path(__file__).parent))

from utils import DataLoader, TextPreprocessor, ResultSaver, compute_salary_annual, extract_competences_from_text

def main():
    """
    Pipeline de preprocessing MASTER
    """
    print("="*70)
    print("ğŸ”§ PREPROCESSING MASTER - POINT UNIQUE DE TRAITEMENT")
    print("="*70)
    
    # ==========================================
    # 1. CHARGEMENT DES DONNÃ‰ES
    # ==========================================
    print("\nğŸ“„ Chargement des donnÃ©es...")
    
    loader = DataLoader()
    df_offres = loader.load_all_offers()
    df_competences = loader.load_competences()
    loader.disconnect()
    
    print(f"\nğŸ“Š Statistiques initiales:")
    print(f"   Total offres: {len(df_offres)}")
    print(f"   Avec description: {df_offres['description'].notna().sum()}")
    print(f"   CompÃ©tences structurÃ©es: {len(df_competences)}")
    
    # ==========================================
    # 2. NETTOYAGE ET ENRICHISSEMENT
    # ==========================================
    print("\nğŸ§¹ Nettoyage des donnÃ©es...")
    
    # Calcul salaire annuel moyen
    df_offres['salary_annual'] = df_offres.apply(compute_salary_annual, axis=1)
    
    # Filtrer les offres avec description
    df_clean = df_offres[df_offres['description'].notna()].copy()
    
    print(f"   Offres aprÃ¨s filtrage: {len(df_clean)}")
    
    # ==========================================
    # 3. PREPROCESSING NLP COMPLET
    # ==========================================
    print("\nğŸ”¤ Preprocessing NLP complet...")
    
    preprocessor = TextPreprocessor(language='french')
    
    # 3.1 Nettoyage HTML
    print("   âœ… Nettoyage HTML (entitÃ©s &nbsp;, balises)...")
    df_clean['description_clean'] = df_clean['description'].apply(
        preprocessor.clean_text
    )
    
    # 3.2 Tokenisation + Stopwords + Lemmatisation
    print("   âœ… Tokenisation + Stopwords FR+EN + Lemmatisation...")
    df_clean['tokens'] = df_clean['description_clean'].apply(
        lambda x: preprocessor.preprocess(x, lemmatize=True)
    )
    
    # 3.3 Texte pour sklearn (rejoint tokens pour TF-IDF/LDA)
    print("   âœ… GÃ©nÃ©ration text_for_sklearn (tokens rejoints)...")
    df_clean['text_for_sklearn'] = df_clean['tokens'].apply(lambda x: ' '.join(x))
    
    # 3.4 Nombre de tokens
    df_clean['num_tokens'] = df_clean['tokens'].apply(len)
    
    print(f"\nğŸ“Š Statistiques texte:")
    print(f"   Tokens moyen par offre: {df_clean['num_tokens'].mean():.0f}")
    print(f"   Tokens mÃ©dian: {df_clean['num_tokens'].median():.0f}")
    print(f"   Tokens min/max: {df_clean['num_tokens'].min()}/{df_clean['num_tokens'].max()}")
    
    # ==========================================
    # 4. VÃ‰RIFICATIONS QUALITÃ‰
    # ==========================================
    print("\nğŸ” VÃ©rifications qualitÃ© preprocessing:")
    
    sample_tokens = df_clean['tokens'].iloc[0]
    print(f"   Exemple tokens (1Ã¨re offre): {sample_tokens[:15]}")
    
    # VÃ©rifier absence stopwords
    from nltk.corpus import stopwords
    stop_fr = set(stopwords.words('french'))
    stop_en = set(stopwords.words('english'))
    
    stopwords_found = [t for t in sample_tokens if t in stop_fr or t in stop_en]
    print(f"   âœ… Stopwords FR+EN filtrÃ©s: {len(stopwords_found) == 0}")
    if stopwords_found:
        print(f"      âš ï¸  TrouvÃ©s: {stopwords_found[:10]}")
    
    # VÃ©rifier absence 'nbsp'
    nbsp_count = sum(1 for tokens in df_clean['tokens'] if 'nbsp' in tokens)
    print(f"   âœ… 'nbsp' supprimÃ©: {nbsp_count == 0} ({nbsp_count} offres contiennent 'nbsp')")
    
    # VÃ©rifier longueur mots >= 3
    short_tokens = [t for t in sample_tokens if len(t) < 3]
    print(f"   âœ… Tokens >= 3 caractÃ¨res: {len(short_tokens) == 0}")
    if short_tokens:
        print(f"      âš ï¸  Tokens courts trouvÃ©s: {short_tokens}")
    
    # ==========================================
    # 5. DICTIONNAIRE COMPÃ‰TENCES
    # ==========================================
    print("\nğŸ“ CrÃ©ation dictionnaire compÃ©tences...")
    
    # CompÃ©tences France Travail
    unique_skills = df_competences['skill_label'].unique()
    print(f"   CompÃ©tences uniques (FT): {len(unique_skills)}")
    
    # CompÃ©tences additionnelles Data/IA
    additional_skills = [
        # Langages
        'Python', 'R', 'SQL', 'Java', 'Scala', 'Julia',
        # ML/DL
        'Machine Learning', 'Deep Learning', 'TensorFlow', 'PyTorch', 
        'Scikit-learn', 'Keras', 'XGBoost', 'LightGBM',
        # Data Engineering
        'Spark', 'Hadoop', 'Kafka', 'Airflow', 'DBT', 'APIs',
        # Cloud
        'AWS', 'Azure', 'GCP', 'Docker', 'Kubernetes',
        # BI
        'Power BI', 'Tableau', 'Looker', 'Qlik',
        # Databases
        'PostgreSQL', 'MySQL', 'MongoDB', 'Cassandra', 'Redis',
        # Libs Python
        'Pandas', 'NumPy', 'Matplotlib', 'Seaborn', 'Plotly',
        # MLOps
        'MLflow', 'Kubeflow', 'MLOps', 'CI/CD', 'Streamlit', 'DevOps', 
        'Databricks', 'Snowflake',
        # NLP
        'NLP', 'NLTK', 'spaCy', 'Transformers', 'BERT', 'GPT',
        'LangChain', 'LlamaIndex',
        # IA /LLM
        'Intelligence Artificielle', 'Large Language Models', 'LLM', 
        'LLMs', 'RAG', 'benchmarks',
        # Other
        'Git', 'Linux', 'API', 'REST', 'GraphQL', 'Agile', 'Scrum'
    ]
    
    # Normaliser tout en lowercase
    all_skills_raw = list(set(list(unique_skills) + additional_skills))
    all_skills = sorted(list(set([skill.lower() for skill in all_skills_raw])))
    
    print(f"   Dictionnaire complet: {len(all_skills)} compÃ©tences (normalisÃ©es lowercase)")
    
    # ==========================================
    # 6. EXTRACTION COMPÃ‰TENCES
    # ==========================================
    print("\nğŸ” Extraction compÃ©tences par pattern matching...")
    
    df_clean['competences_found'] = df_clean['description'].apply(
        lambda x: extract_competences_from_text(x, all_skills)
    )
    
    df_clean['num_competences'] = df_clean['competences_found'].apply(len)
    
    print(f"   CompÃ©tences moyennes par offre: {df_clean['num_competences'].mean():.1f}")
    print(f"   Offres avec compÃ©tences: {(df_clean['num_competences'] > 0).sum()}")
    
    # Top compÃ©tences
    from collections import Counter
    all_comps = [comp for comps in df_clean['competences_found'] for comp in comps]
    comp_counter = Counter(all_comps)
    
    print(f"\nğŸ† Top 10 compÃ©tences extraites:")
    for comp, count in comp_counter.most_common(10):
        pct = count / len(df_clean) * 100
        print(f"   {comp:<30s}: {count:4d} ({pct:5.1f}%)")
    
    # ==========================================
    # 7. STATISTIQUES PAR SOURCE
    # ==========================================
    print("\nğŸ“Š Statistiques par source:")
    
    for source in df_clean['source_name'].unique():
        df_source = df_clean[df_clean['source_name'] == source]
        print(f"\n   {source}:")
        print(f"      Offres: {len(df_source)}")
        print(f"      Tokens moyen: {df_source['num_tokens'].mean():.0f}")
        print(f"      CompÃ©tences moyennes: {df_source['num_competences'].mean():.1f}")
        print(f"      Avec salaire: {df_source['salary_annual'].notna().sum()}")
        print(f"      RÃ©gions uniques: {df_source['region'].nunique()}")
    
    # ==========================================
    # 8. STATISTIQUES PAR RÃ‰GION
    # ==========================================
    print("\nğŸ—ºï¸  Top 10 rÃ©gions:")
    
    region_stats = df_clean.groupby('region').agg({
        'offre_id': 'count',
        'salary_annual': 'median',
        'num_tokens': 'mean',
        'num_competences': 'mean'
    }).round(1)
    region_stats.columns = ['nb_offres', 'salaire_median', 'tokens_moyen', 'comp_moyennes']
    region_stats = region_stats.sort_values('nb_offres', ascending=False).head(10)
    
    print(region_stats)
    
    # ==========================================
    # 9. SAUVEGARDE DATA_CLEAN.PKL (COMPLET)
    # ==========================================
    print("\nğŸ’¾ Sauvegarde data_clean.pkl (COMPLET)...")
    
    saver = ResultSaver()
    
    # Colonnes Ã  garder
    colonnes_finales = [
        # IDs
        'offre_id', 'job_id_source',
        # MÃ©tadonnÃ©es
        'source_name', 'title', 'company_name',
        'city', 'department', 'region', 'latitude', 'longitude',
        'contract_type', 'experience_level', 'duration',
        'salary_min', 'salary_max', 'salary_annual', 'salary_text',
        'date_posted', 'url', 'scraped_at',
        # Textes (TOUS les formats)
        'description',           # Texte brut original
        'description_clean',     # HTML nettoyÃ©
        'tokens',                # Liste tokens (stopwords filtrÃ©s, lemmatisÃ©s)
        'text_for_sklearn',      # String pour TF-IDF/LDA
        'num_tokens',            # Nombre tokens
        # CompÃ©tences
        'competences_found',     # Liste compÃ©tences extraites
        'num_competences'        # Nombre compÃ©tences
    ]
    
    df_final = df_clean[colonnes_finales].copy()
    
    print(f"\nğŸ“‹ Colonnes dans data_clean.pkl:")
    for col in colonnes_finales:
        print(f"   - {col}")
    
    # Sauvegarder
    saver.save_pickle(df_final, 'data_clean.pkl')
    
    # Export CSV (sans tokens pour lisibilitÃ©)
    df_export = df_final.drop(columns=['tokens', 'competences_found'], errors='ignore')
    saver.save_csv(df_export, 'data_clean.csv')
    
    # Sauvegarder dictionnaire compÃ©tences
    saver.save_json({'competences': all_skills}, 'dictionnaire_competences.json')
    
    # Sauvegarder compÃ©tences FT
    saver.save_pickle(df_competences, 'competences_ft.pkl')
    
    # ==========================================
    # 10. STATISTIQUES GLOBALES
    # ==========================================
    print("\nğŸ“Š GÃ©nÃ©ration statistiques globales...")
    
    stats = {
        'total_offres': len(df_final),
        'sources': df_final['source_name'].value_counts().to_dict(),
        'regions': df_final['region'].value_counts().head(10).to_dict(),
        'contract_types': df_final['contract_type'].value_counts().to_dict(),
        'salary_stats': {
            'count': int(df_final['salary_annual'].notna().sum()),
            'mean': float(df_final['salary_annual'].mean()) if df_final['salary_annual'].notna().any() else 0,
            'median': float(df_final['salary_annual'].median()) if df_final['salary_annual'].notna().any() else 0,
            'min': float(df_final['salary_annual'].min()) if df_final['salary_annual'].notna().any() else 0,
            'max': float(df_final['salary_annual'].max()) if df_final['salary_annual'].notna().any() else 0
        },
        'text_stats': {
            'tokens_mean': float(df_final['num_tokens'].mean()),
            'tokens_median': float(df_final['num_tokens'].median()),
            'tokens_min': int(df_final['num_tokens'].min()),
            'tokens_max': int(df_final['num_tokens'].max())
        },
        'competences_stats': {
            'dictionnaire_size': len(all_skills),
            'comp_mean': float(df_final['num_competences'].mean()),
            'comp_median': float(df_final['num_competences'].median()),
            'offers_with_comp': int((df_final['num_competences'] > 0).sum()),
            'top_10': [
                {'competence': c, 'count': int(n), 'percentage': float(n/len(df_final)*100)}
                for c, n in comp_counter.most_common(10)
            ]
        },
        'preprocessing': {
            'stopwords_count': len(preprocessor.stop_words),
            'stopwords_fr_en': True,
            'lemmatization': True,
            'html_cleaning': True,
            'min_token_length': 3
        },
        'timestamp': pd.Timestamp.now().isoformat()
    }
    
    saver.save_json(stats, 'stats_globales.json')
    
    # ==========================================
    # RÃ‰SUMÃ‰ FINAL
    # ==========================================
    print("\n" + "="*70)
    print("âœ… PREPROCESSING MASTER TERMINÃ‰ !")
    print("="*70)
    
    print(f"\nğŸ“ Fichier principal crÃ©Ã©:")
    print(f"   ğŸ“¦ data_clean.pkl ({len(df_final)} offres, {len(colonnes_finales)} colonnes)")
    
    print(f"\nğŸ“‹ Colonnes disponibles pour analyses:")
    print(f"   ğŸ“ description          â†’ Texte brut original")
    print(f"   ğŸ§¹ description_clean    â†’ HTML nettoyÃ©")
    print(f"   ğŸ”¤ tokens               â†’ Liste tokens (clean, lemmatisÃ©s)")
    print(f"   ğŸ“Š text_for_sklearn     â†’ String pour TF-IDF/LDA")
    print(f"   ğŸ“ competences_found    â†’ CompÃ©tences extraites")
    
    print(f"\nğŸ“Š QualitÃ© du preprocessing:")
    print(f"   âœ… Stopwords FR+EN filtrÃ©s ({len(preprocessor.stop_words)} stopwords)")
    print(f"   âœ… Lemmatisation appliquÃ©e (technique/techniques â†’ technique)")
    print(f"   âœ… HTML nettoyÃ© (&nbsp; supprimÃ©)")
    print(f"   âœ… Tokens >= 3 caractÃ¨res")
    print(f"   âœ… CompÃ©tences normalisÃ©es (lowercase)")
    
    print(f"\nğŸ“‚ Fichiers additionnels:")
    print(f"   - data_clean.csv (export sans tokens)")
    print(f"   - dictionnaire_competences.json ({len(all_skills)} compÃ©tences)")
    print(f"   - competences_ft.pkl (compÃ©tences France Travail)")
    print(f"   - stats_globales.json (statistiques complÃ¨tes)")
    
    print(f"\nğŸš€ Prochaines Ã©tapes:")
    print(f"   1. python 2_extraction_competences_v2.py")
    print(f"   2. python 3_topic_modeling_v2.py")
    
    return df_final, all_skills


if __name__ == "__main__":
    df_clean, skills_dict = main()