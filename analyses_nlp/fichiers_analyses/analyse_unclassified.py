"""
Analyse des Titres Non Classifi√©s
Identifie les patterns manquants dans la classification

Auteur: Projet NLP Text Mining
Date: D√©cembre 2025
"""

import pandas as pd
import pickle
from pathlib import Path
from collections import Counter
import re


def load_data():
    """Charge data_with_profiles.pkl"""
    print("="*70)
    print("üìä ANALYSE DES TITRES NON CLASSIFI√âS")
    print("="*70)
    
    pkl_path = Path('../resultats_nlp/models/data_with_profiles.pkl')
    
    print(f"\nüì• Chargement {pkl_path}...")
    
    with open(pkl_path, 'rb') as f:
        df = pickle.load(f)
    
    print(f"   ‚úÖ Offres charg√©es: {len(df)}")
    
    return df


def analyze_classification_stats(df):
    """Statistiques g√©n√©rales de classification"""
    print("\n" + "="*70)
    print("üìà STATISTIQUES G√âN√âRALES")
    print("="*70)
    
    n_total = len(df)
    n_classified = (df['profil_assigned'] != 'Non classifi√©').sum()
    n_unclassified = (df['profil_assigned'] == 'Non classifi√©').sum()
    
    print(f"\nTotal offres:     {n_total:,}")
    print(f"Classifi√©es:      {n_classified:,} ({n_classified/n_total*100:.1f}%)")
    print(f"Non classifi√©es:  {n_unclassified:,} ({n_unclassified/n_total*100:.1f}%)")
    
    print("\nüìä Distribution profils classifi√©s:")
    profil_counts = df[df['profil_assigned'] != 'Non classifi√©']['profil_assigned'].value_counts()
    
    for profil, count in profil_counts.items():
        pct = count / n_total * 100
        print(f"   {profil:<30s}: {count:4d} ({pct:5.1f}%)")


def analyze_unclassified_titles(df):
    """Analyse des titres non classifi√©s"""
    print("\n" + "="*70)
    print("üîç TOP 50 TITRES NON CLASSIFI√âS")
    print("="*70)
    
    df_unclassified = df[df['profil_assigned'] == 'Non classifi√©'].copy()
    
    print(f"\nTotal non classifi√©s: {len(df_unclassified):,}")
    
    # Top 50 titres
    title_counts = df_unclassified['title'].value_counts().head(50)
    
    print(f"\nTop 50 titres (repr√©sentent {title_counts.sum()} offres):\n")
    
    for i, (title, count) in enumerate(title_counts.items(), 1):
        print(f"{i:2d}. [{count:3d}x] {title}")
    
    return df_unclassified, title_counts


def extract_keywords_from_titles(df_unclassified):
    """Extrait les mots-cl√©s fr√©quents des titres non classifi√©s"""
    print("\n" + "="*70)
    print("üîë MOTS-CL√âS FR√âQUENTS DANS TITRES NON CLASSIFI√âS")
    print("="*70)
    
    # Tous les titres en lowercase
    all_titles = ' '.join(df_unclassified['title'].fillna('').str.lower())
    
    # Extraire mots (au moins 4 lettres)
    words = re.findall(r'\b[a-z√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ß]{4,}\b', all_titles)
    
    # Stopwords basiques
    stopwords = {
        'dans', 'pour', 'avec', 'chez', 'vers', 'sous', 'sans',
        'stage', 'alternance', 'cdi', 'cdd', 'apprentissage',
        'junior', 'senior', 'lead', 'expert',
        'paris', 'lyon', 'marseille', 'toulouse', 'bordeaux',
        'france', 'remote', 't√©l√©travail', 'teletravail'
    }
    
    # Filtrer stopwords
    words_filtered = [w for w in words if w not in stopwords]
    
    # Compter
    word_counts = Counter(words_filtered)
    
    print("\nTop 30 mots-cl√©s:")
    for i, (word, count) in enumerate(word_counts.most_common(30), 1):
        print(f"{i:2d}. {word:<20s}: {count:4d}x")
    
    return word_counts


def identify_patterns(title_counts):
    """Identifie les patterns de titres"""
    print("\n" + "="*70)
    print("üéØ PATTERNS IDENTIFI√âS")
    print("="*70)
    
    patterns = {
        'Ing√©nieur': [],
        'D√©veloppeur': [],
        'Analyste': [],
        'Consultant': [],
        'Architecte': [],
        'Chef de projet': [],
        'Product': [],
        'Manager': [],
        'Autres': []
    }
    
    for title, count in title_counts.items():
        title_lower = title.lower()
        
        matched = False
        for pattern in patterns.keys():
            if pattern.lower() in title_lower:
                patterns[pattern].append((title, count))
                matched = True
                break
        
        if not matched:
            patterns['Autres'].append((title, count))
    
    # Afficher patterns
    for pattern, titles in patterns.items():
        if len(titles) > 0:
            total = sum(count for _, count in titles)
            print(f"\nüìå {pattern} ({len(titles)} titres uniques, {total} offres):")
            
            # Top 10 de ce pattern
            for title, count in sorted(titles, key=lambda x: x[1], reverse=True)[:10]:
                print(f"   [{count:3d}x] {title}")


def analyze_scores_unclassified(df):
    """Analyse les scores des offres non classifi√©es"""
    print("\n" + "="*70)
    print("üìä ANALYSE SCORES OFFRES NON CLASSIFI√âES")
    print("="*70)
    
    df_unclassified = df[df['profil_assigned'] == 'Non classifi√©'].copy()
    
    print(f"\nScore moyen:    {df_unclassified['profil_score'].mean():.2f}/10")
    print(f"Score m√©dian:   {df_unclassified['profil_score'].median():.2f}/10")
    print(f"Score min:      {df_unclassified['profil_score'].min():.2f}/10")
    print(f"Score max:      {df_unclassified['profil_score'].max():.2f}/10")
    
    # Distribution scores
    print("\nDistribution scores:")
    bins = [0, 2, 3, 4, 5, 6, 7, 8, 10]
    labels = ['0-2', '2-3', '3-4', '4-5', '5-6', '6-7', '7-8', '8-10']
    
    df_unclassified['score_bin'] = pd.cut(
        df_unclassified['profil_score'], 
        bins=bins, 
        labels=labels,
        include_lowest=True
    )
    
    score_dist = df_unclassified['score_bin'].value_counts().sort_index()
    
    for score_range, count in score_dist.items():
        pct = count / len(df_unclassified) * 100
        bar = '‚ñà' * int(pct / 2)
        print(f"   {score_range}: {count:4d} ({pct:5.1f}%) {bar}")
    
    # Profils second (qui auraient pu matcher)
    print("\nü•à Top profils en 2√®me position (qui auraient pu matcher):")
    second_counts = df_unclassified['profil_second'].value_counts().head(10)
    
    for profil, count in second_counts.items():
        pct = count / len(df_unclassified) * 100
        print(f"   {profil:<30s}: {count:4d} ({pct:5.1f}%)")


def analyze_by_source(df):
    """Analyse par source"""
    print("\n" + "="*70)
    print("üìç ANALYSE PAR SOURCE")
    print("="*70)
    
    df_unclassified = df[df['profil_assigned'] == 'Non classifi√©'].copy()
    
    print("\nTaux de non-classification par source:")
    
    for source in df['source_name'].unique():
        df_source = df[df['source_name'] == source]
        df_source_unclass = df_unclassified[df_unclassified['source_name'] == source]
        
        n_total = len(df_source)
        n_unclass = len(df_source_unclass)
        pct = n_unclass / n_total * 100 if n_total > 0 else 0
        
        print(f"   {source:<20s}: {n_unclass:4d}/{n_total:4d} ({pct:5.1f}%)")


def generate_recommendations(df, title_counts, word_counts):
    """G√©n√®re des recommandations"""
    print("\n" + "="*70)
    print("üí° RECOMMANDATIONS")
    print("="*70)
    
    df_unclassified = df[df['profil_assigned'] == 'Non classifi√©'].copy()
    
    # Analyser les mots-cl√©s manquants
    print("\n1Ô∏è‚É£ PROFILS √Ä AJOUTER/ENRICHIR:")
    
    # Identifier patterns
    common_words = [w for w, c in word_counts.most_common(20)]
    
    suggestions = {
        'Data Engineer': ['ing√©nieur', 'd√©veloppeur', 'data', 'big', 'cloud', 'plateforme'],
        'Data Scientist': ['scientist', 'scientifique', 'machine', 'learning', 'intelligence'],
        'Data Analyst': ['analyste', 'analyst', 'business', 'reporting'],
        'BI Analyst': ['tableau', 'power', 'looker', 'qlik', 'business intelligence'],
        'DevOps/MLOps': ['devops', 'mlops', 'kubernetes', 'docker', 'infrastructure'],
        'Product Manager': ['product', 'manager', 'chef', 'projet'],
        'Autres profils': ['architecte', 'consultant', 'lead', 'expert']
    }
    
    for profil, keywords in suggestions.items():
        matches = [w for w in common_words if any(k in w for k in keywords)]
        if matches:
            print(f"\n   üìå {profil}:")
            print(f"      Mots-cl√©s trouv√©s: {', '.join(matches[:5])}")
    
    # Variantes √† ajouter
    print("\n2Ô∏è‚É£ VARIANTES DE TITRES √Ä AJOUTER:")
    
    print("\n   Exemples de titres fr√©quents non match√©s:")
    for i, (title, count) in enumerate(list(title_counts.items())[:15], 1):
        print(f"   {i:2d}. [{count:3d}x] {title}")
    
    # Seuils
    print("\n3Ô∏è‚É£ AJUSTEMENT SEUILS:")
    
    score_stats = df_unclassified['profil_score'].describe()
    
    print(f"\n   Score m√©dian non classifi√©s: {score_stats['50%']:.2f}/10")
    print(f"   Score Q3 non classifi√©s:     {score_stats['75%']:.2f}/10")
    
    if score_stats['75%'] >= 4.5:
        print(f"\n   ‚ö†Ô∏è 75% des non-classifi√©s ont score ‚â• {score_stats['75%']:.2f}")
        print(f"   ‚Üí Recommandation: Baisser min_score √† {score_stats['75%']:.2f}")


def save_results(df, title_counts, word_counts):
    """Sauvegarde les r√©sultats"""
    print("\n" + "="*70)
    print("üíæ SAUVEGARDE R√âSULTATS")
    print("="*70)
    
    output_dir = Path('../resultats_nlp')
    
    # 1. Top titres non classifi√©s
    df_top_titles = pd.DataFrame({
        'titre': title_counts.index,
        'count': title_counts.values
    })
    
    output_file = output_dir / 'titres_non_classifies.csv'
    df_top_titles.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\n‚úÖ {output_file}")
    
    # 2. Mots-cl√©s fr√©quents
    df_keywords = pd.DataFrame({
        'mot_cle': [w for w, c in word_counts.most_common(100)],
        'count': [c for w, c in word_counts.most_common(100)]
    })
    
    output_file = output_dir / 'mots_cles_titres.csv'
    df_keywords.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"‚úÖ {output_file}")
    
    # 3. √âchantillon offres non classifi√©es
    df_unclassified = df[df['profil_assigned'] == 'Non classifi√©'].copy()
    
    df_sample = df_unclassified[[
        'title', 'profil_score', 'profil_second', 'profil_second_score',
        'score_title', 'score_description', 'score_competences',
        'region', 'source_name'
    ]].head(200)
    
    output_file = output_dir / 'echantillon_non_classifies.csv'
    df_sample.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"‚úÖ {output_file}")


def main():
    """Pipeline analyse"""
    
    # Charger donn√©es
    df = load_data()
    
    # Stats g√©n√©rales
    analyze_classification_stats(df)
    
    # Titres non classifi√©s
    df_unclassified, title_counts = analyze_unclassified_titles(df)
    
    # Mots-cl√©s
    word_counts = extract_keywords_from_titles(df_unclassified)
    
    # Patterns
    identify_patterns(title_counts)
    
    # Scores
    analyze_scores_unclassified(df)
    
    # Par source
    analyze_by_source(df)
    
    # Recommandations
    generate_recommendations(df, title_counts, word_counts)
    
    # Sauvegarder
    save_results(df, title_counts, word_counts)
    
    print("\n" + "="*70)
    print("‚úÖ ANALYSE TERMIN√âE !")
    print("="*70)
    print("\nüìÅ Fichiers cr√©√©s:")
    print("   - titres_non_classifies.csv")
    print("   - mots_cles_titres.csv")
    print("   - echantillon_non_classifies.csv")
    print("\nüí° Utilise ces insights pour ajuster les profils et variantes !")


if __name__ == "__main__":
    main()