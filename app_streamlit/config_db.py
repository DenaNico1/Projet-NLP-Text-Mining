"""
Configuration Base de Donn√©es PostgreSQL (Supabase + Docker Local)
Projet NLP Text Mining - Master SISE

Connexion intelligente avec basculement automatique :
- Supabase (online) par d√©faut
- Docker PostgreSQL local (offline) en fallback

Mod√®le en √©toile : 5 dimensions + 2 faits
"""

import os
import psycopg2
import pandas as pd
import streamlit as st
from dotenv import load_dotenv

# Charger variables d'environnement
load_dotenv()

# ============================================
# CONFIGURATION
# ============================================

# Configuration Supabase (production/online)
SUPABASE_CONFIG = {
    'host': os.getenv('DB_HOST', 'aws-1-eu-north-1.pooler.supabase.com'),
    'port': int(os.getenv('DB_PORT', 5432)),
    'database': os.getenv('DB_NAME', 'postgres'),
    'user': os.getenv('DB_USER', 'postgres.znkulobexqmrshfkgynv'),
    'password': os.getenv('DB_PASSWORD', '')
}

# Configuration Docker local (offline/d√©veloppement)
DOCKER_CONFIG = {
    'host': 'postgres' if os.getenv('DOCKER_ENV') == 'true' else 'localhost',
    'port': 5432,
    'database': 'entrepot_nlp',
    'user': 'nlp_user',
    'password': 'nlp_password_2026'
}

# Mode forc√© via variable d'environnement (USE_LOCAL_DB=true pour forcer Docker)
USE_LOCAL_DB = os.getenv('USE_LOCAL_DB', 'false').lower() == 'true'

def test_connection(config, timeout=3):
    """Test rapide de connexion PostgreSQL"""
    try:
        conn = psycopg2.connect(**config, connect_timeout=timeout)
        conn.close()
        return True
    except:
        return False

def get_active_config():
    """
    D√©termine quelle configuration utiliser
    1. Si USE_LOCAL_DB=true ‚Üí Docker
    2. Si Supabase accessible ‚Üí Supabase
    3. Sinon ‚Üí Docker (fallback)
    """
    if USE_LOCAL_DB:
        return DOCKER_CONFIG, 'local'
    
    # Tester Supabase d'abord
    if SUPABASE_CONFIG['password'] and test_connection(SUPABASE_CONFIG):
        return SUPABASE_CONFIG, 'supabase'
    
    # Fallback sur Docker local
    if test_connection(DOCKER_CONFIG):
        return DOCKER_CONFIG, 'local'
    
    # Par d√©faut, retourner Supabase (m√™me si non accessible)
    return SUPABASE_CONFIG, 'supabase'

# Obtenir la config active
DB_CONFIG, DB_MODE = get_active_config()

# ============================================
# CONNEXION
# ============================================

@st.cache_resource
def get_db_connection():
    """
    Cr√©e et cache une connexion PostgreSQL
    Bascule automatiquement entre Supabase et Docker local
    """
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        
        # Afficher mode actif dans la sidebar
        if DB_MODE == 'supabase':
            st.sidebar.success("üåê Connect√© √† Supabase (online)")
        else:
            st.sidebar.info("üíª Connect√© √† Docker local (offline)")
        
        return conn
    except Exception as e:
        st.error(f"‚ùå Erreur connexion PostgreSQL ({DB_MODE}): {e}")
        
        if DB_MODE == 'supabase':
            st.info("üí° V√©rifiez votre connexion internet ou passez en mode local")
            st.code("USE_LOCAL_DB=true")
        else:
            st.info("üí° Assurez-vous que Docker est d√©marr√©")
            st.code("docker-compose up -d postgres")
        
        return None

# ============================================
# CHARGEMENT DONN√âES (VUE COMPL√àTE)
# ============================================

@st.cache_data(ttl=300)  # Cache 5 minutes
def load_offres_complete():
    """
    Charge toutes les offres via la vue v_offres_complete
    (sans r√©sultats NLP)
    
    Returns:
        pd.DataFrame: Offres avec toutes dimensions jointes
    """
    conn = get_db_connection()
    if not conn:
        return pd.DataFrame()
    
    query = """
    SELECT 
        o.offre_id,
        o.job_id_source,
        s.source_name as source,
        o.title,
        e.company_name,
        l.city,
        l.department,
        l.region,
        l.latitude,
        l.longitude,
        c.contract_type,
        c.experience_level,
        o.salary_min,
        o.salary_max,
        t.date_posted,
        o.description,
        o.url,
        o.scraped_at
    FROM fact_offres o
    LEFT JOIN dim_source s ON o.source_id = s.source_id
    LEFT JOIN dim_localisation l ON o.localisation_id = l.localisation_id
    LEFT JOIN dim_entreprise e ON o.entreprise_id = e.entreprise_id
    LEFT JOIN dim_contrat c ON o.contrat_id = c.contrat_id
    LEFT JOIN dim_temps t ON o.temps_id = t.temps_id
    """
    
    try:
        df = pd.read_sql(query, conn)
        
        # Calculer salary_annual (moyenne min/max) pour compatibilit√©
        df['salary_annual'] = df[['salary_min', 'salary_max']].mean(axis=1)
        
        return df
    except Exception as e:
        st.error(f"Erreur chargement offres: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=300)  # Cache 5 minutes
def load_offres_with_nlp():
    """
    ‚≠ê FONCTION PRINCIPALE APR√àS MIGRATION NLP COMPL√àTE
    
    Charge toutes les offres avec r√©sultats NLP (profils, topics, clusters)
    Via la vue v_offres_nlp_complete
    
    Cette fonction REMPLACE l'ancien pickle data_with_profiles.pkl
    100% COMPATIBLE - Toutes colonnes pickle incluses
    
    Returns:
        pd.DataFrame: Offres compl√®tes avec r√©sultats NLP (38 colonnes)
            Colonnes BASE:
            - offre_id, title, company_name, city, region, etc.
            - salary_min, salary_max, salary_annual
            - latitude, longitude, contract_type, etc.
            
            Colonnes NLP PRINCIPALES:
            - status, profil_assigned, score_classification
            - competences_found, topic_id, cluster_id
            
            Colonnes NLP D√âTAILL√âES:
            - num_tokens, num_competences
            - profil_score, profil_confidence, profil_second, profil_second_score
            - score_title, score_description, score_competences
            - cascade_pass
            
            Colonnes PREPROCESSING:
            - description_clean, text_for_sklearn, tokens
            
            Colonnes SUPPL√âMENTAIRES:
            - duration, salary_text, source_name
    """
    conn = get_db_connection()
    if not conn:
        return pd.DataFrame()
    
    # Utiliser directement la vue v_offres_nlp_complete qui contient TOUTES les colonnes
    query = "SELECT * FROM v_offres_nlp_complete"
    
    try:
        df = pd.read_sql(query, conn)
        return df
    except Exception as e:
        st.error(f"Erreur chargement offres avec NLP: {e}")
        return pd.DataFrame()

# ============================================
# CHARGEMENT DONN√âES AVEC COMP√âTENCES
# ============================================

@st.cache_data(ttl=300)
def load_offres_with_competences():
    """
    Charge offres avec leurs comp√©tences
    
    Returns:
        pd.DataFrame: Offres avec colonne 'competences_found' (liste)
    """
    conn = get_db_connection()
    if not conn:
        return pd.DataFrame()
    
    query = """
    SELECT 
        o.offre_id,
        o.title,
        e.company_name,
        l.city,
        l.region,
        s.source_name as source,
        ARRAY_AGG(DISTINCT c.skill_label) FILTER (WHERE c.skill_label IS NOT NULL) as competences_found
    FROM fact_offres o
    LEFT JOIN dim_source s ON o.source_id = s.source_id
    LEFT JOIN dim_localisation l ON o.localisation_id = l.localisation_id
    LEFT JOIN dim_entreprise e ON o.entreprise_id = e.entreprise_id
    LEFT JOIN fact_competences c ON o.offre_id = c.offre_id
    GROUP BY o.offre_id, o.title, e.company_name, l.city, l.region, s.source_name
    """
    
    try:
        df = pd.read_sql(query, conn)
        return df
    except Exception as e:
        st.error(f"Erreur chargement comp√©tences: {e}")
        return pd.DataFrame()

# ============================================
# REQU√äTES SP√âCIFIQUES
# ============================================

def load_offres_by_region(region):
    """Charge offres d'une r√©gion sp√©cifique"""
    conn = get_db_connection()
    if not conn:
        return pd.DataFrame()
    
    query = """
    SELECT o.*, l.region, e.company_name
    FROM fact_offres o
    JOIN dim_localisation l ON o.localisation_id = l.localisation_id
    JOIN dim_entreprise e ON o.entreprise_id = e.entreprise_id
    WHERE l.region = %s
    """
    
    try:
        df = pd.read_sql(query, conn, params=(region,))
        return df
    except Exception as e:
        st.error(f"Erreur chargement r√©gion: {e}")
        return pd.DataFrame()

def load_stats_regions():
    """Charge statistiques par r√©gion (vue pr√©-calcul√©e)"""
    conn = get_db_connection()
    if not conn:
        return pd.DataFrame()
    
    query = "SELECT * FROM v_stats_region"
    
    try:
        df = pd.read_sql(query, conn)
        return df
    except Exception as e:
        st.error(f"Erreur stats r√©gions: {e}")
        return pd.DataFrame()

def get_top_competences(limit=20):
    """Top N comp√©tences les plus demand√©es"""
    conn = get_db_connection()
    if not conn:
        return pd.DataFrame()
    
    query = """
    SELECT 
        skill_label,
        COUNT(*) as nb_offres
    FROM fact_competences
    GROUP BY skill_label
    ORDER BY nb_offres DESC
    LIMIT %s
    """
    
    try:
        df = pd.read_sql(query, conn, params=(limit,))
        return df
    except Exception as e:
        st.error(f"Erreur top comp√©tences: {e}")
        return pd.DataFrame()

def get_top_entreprises(limit=20):
    """Top N entreprises qui recrutent le plus"""
    conn = get_db_connection()
    if not conn:
        return pd.DataFrame()
    
    query = """
    SELECT 
        e.company_name,
        COUNT(o.offre_id) as nb_offres
    FROM fact_offres o
    JOIN dim_entreprise e ON o.entreprise_id = e.entreprise_id
    GROUP BY e.company_name
    ORDER BY nb_offres DESC
    LIMIT %s
    """
    
    try:
        df = pd.read_sql(query, conn, params=(limit,))
        return df
    except Exception as e:
        st.error(f"Erreur top entreprises: {e}")
        return pd.DataFrame()

# ============================================
# AJOUT NOUVELLE OFFRE (pour future impl√©mentation)
# ============================================

def add_offre(offre_data):
    """
    Ajoute une nouvelle offre dans l'entrep√¥t
    
    Args:
        offre_data (dict): Donn√©es de l'offre
            {
                'job_id_source': str,
                'title': str,
                'company_name': str,
                'city': str,
                'region': str,
                'contract_type': str,
                'description': str,
                'url': str,
                'source': str,
                ...
            }
    
    Returns:
        int: ID de l'offre cr√©√©e, ou None si erreur
    """
    conn = get_db_connection()
    if not conn:
        return None
    
    cursor = conn.cursor()
    
    try:
        # 1. R√©cup√©rer ou cr√©er source_id
        cursor.execute(
            "SELECT source_id FROM dim_source WHERE source_name = %s",
            (offre_data.get('source', 'Manuel'),)
        )
        result = cursor.fetchone()
        if result:
            source_id = result[0]
        else:
            cursor.execute(
                "INSERT INTO dim_source (source_name) VALUES (%s) RETURNING source_id",
                (offre_data.get('source', 'Manuel'),)
            )
            source_id = cursor.fetchone()[0]
        
        # 2. R√©cup√©rer ou cr√©er entreprise_id
        cursor.execute(
            "SELECT entreprise_id FROM dim_entreprise WHERE company_name = %s",
            (offre_data.get('company_name', 'Non sp√©cifi√©'),)
        )
        result = cursor.fetchone()
        if result:
            entreprise_id = result[0]
        else:
            cursor.execute(
                "INSERT INTO dim_entreprise (company_name) VALUES (%s) RETURNING entreprise_id",
                (offre_data.get('company_name', 'Non sp√©cifi√©'),)
            )
            entreprise_id = cursor.fetchone()[0]
        
        # 3. Ins√©rer offre (simplifi√© - √† compl√©ter avec autres dimensions)
        cursor.execute("""
            INSERT INTO fact_offres (
                source_id, entreprise_id, job_id_source, title, description, url, scraped_at
            ) VALUES (%s, %s, %s, %s, %s, %s, NOW())
            RETURNING offre_id
        """, (
            source_id,
            entreprise_id,
            offre_data.get('job_id_source', ''),
            offre_data.get('title', ''),
            offre_data.get('description', ''),
            offre_data.get('url', '')
        ))
        
        offre_id = cursor.fetchone()[0]
        conn.commit()
        
        # Invalider cache Streamlit
        st.cache_data.clear()
        
        return offre_id
        
    except Exception as e:
        conn.rollback()
        st.error(f"Erreur ajout offre: {e}")
        return None
    finally:
        cursor.close()

# ============================================
# UTILITAIRES
# ============================================

def test_connection():
    """Teste la connexion PostgreSQL"""
    conn = get_db_connection()
    if conn:
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM fact_offres")
            count = cursor.fetchone()[0]
            cursor.close()
            return True, f"‚úÖ Connexion OK - {count} offres en base"
        except Exception as e:
            return False, f"‚ùå Erreur requ√™te: {e}"
    else:
        return False, "‚ùå Connexion impossible"

def get_db_stats():
    """Statistiques globales base de donn√©es"""
    conn = get_db_connection()
    if not conn:
        return {}
    
    stats = {}
    tables = ['dim_source', 'dim_localisation', 'dim_entreprise', 
              'dim_contrat', 'dim_temps', 'fact_offres', 'fact_competences']
    
    try:
        cursor = conn.cursor()
        for table in tables:
            cursor.execute(f"SELECT COUNT(*) FROM {table}")
            stats[table] = cursor.fetchone()[0]
        cursor.close()
        return stats
    except Exception as e:
        st.error(f"Erreur stats: {e}")
        return {}