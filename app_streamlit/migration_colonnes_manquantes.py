"""
MIGRATION COLONNES MANQUANTES - PICKLE ‚Üí POSTGRESQL
Projet NLP Text Mining - Master SISE

Ajoute 16 colonnes manquantes dans fact_nlp_analysis
pour compatibilit√© 100% avec data_with_profiles.pkl
"""

import pickle
import psycopg2
from psycopg2.extras import execute_values
import pandas as pd
from tqdm import tqdm
import sys
from pathlib import Path
import os
from dotenv import load_dotenv

load_dotenv()

print("="*80)
print("üîß AJOUT COLONNES MANQUANTES ‚Üí POSTGRESQL")
print("="*80)

# ============================================
# CONFIGURATION
# ============================================

PICKLE_PATH = Path('../resultats_nlp/models/data_with_profiles.pkl')

DB_CONFIG = {
    'host': os.getenv('DB_HOST'),
    'port': int(os.getenv('DB_PORT', 5432)),
    'database': os.getenv('DB_NAME'),
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD')
}

# Colonnes √† ajouter
COLONNES_MANQUANTES = [
    ('num_tokens', 'INTEGER'),
    ('num_competences', 'INTEGER'),
    ('profil_score', 'NUMERIC(5,2)'),
    ('profil_confidence', 'NUMERIC(5,2)'),
    ('profil_second', 'VARCHAR(100)'),
    ('profil_second_score', 'NUMERIC(5,2)'),
    ('score_title', 'NUMERIC(5,2)'),
    ('score_description', 'NUMERIC(5,2)'),
    ('score_competences', 'NUMERIC(5,2)'),
    ('cascade_pass', 'INTEGER'),
    ('description_clean', 'TEXT'),
    ('text_for_sklearn', 'TEXT'),
    ('tokens', 'TEXT'),
    ('duration', 'VARCHAR(100)'),
    ('salary_text', 'VARCHAR(200)'),
    ('source_name', 'VARCHAR(50)')
]

print(f"\n‚úÖ {len(COLONNES_MANQUANTES)} colonnes √† ajouter")

# ============================================
# CHARGEMENT PICKLE
# ============================================

print("\n" + "="*80)
print("üì• CHARGEMENT PICKLE")
print("="*80)

try:
    print(f"\n   Lecture: {PICKLE_PATH}")
    with open(PICKLE_PATH, 'rb') as f:
        df_pickle = pickle.load(f)
    
    print(f"   ‚úÖ {len(df_pickle)} lignes charg√©es")
    
except Exception as e:
    print(f"\n‚ùå Erreur: {e}")
    sys.exit(1)

# V√©rifier offre_id existe
if 'offre_id' not in df_pickle.columns:
    print("\n‚ö†Ô∏è  Cr√©ation colonne offre_id...")
    df_pickle['offre_id'] = df_pickle.index + 1

# Conversion NaN ‚Üí None
df_pickle = df_pickle.where(pd.notna(df_pickle), None)

# ============================================
# CONNEXION POSTGRESQL
# ============================================

print("\n" + "="*80)
print("üîó CONNEXION POSTGRESQL")
print("="*80)

try:
    print(f"\n   Host: {DB_CONFIG['host']}")
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    print("   ‚úÖ Connect√©")
    
except Exception as e:
    print(f"\n‚ùå Erreur connexion: {e}")
    sys.exit(1)

# ============================================
# AJOUT COLONNES DANS TABLE
# ============================================

print("\n" + "="*80)
print("üèóÔ∏è  AJOUT COLONNES DANS fact_nlp_analysis")
print("="*80)

added_cols = []
existing_cols = []

for col_name, col_type in COLONNES_MANQUANTES:
    try:
        print(f"\n   Ajout {col_name} ({col_type})...", end=" ")
        
        cursor.execute(f"""
            ALTER TABLE fact_nlp_analysis 
            ADD COLUMN IF NOT EXISTS {col_name} {col_type}
        """)
        
        conn.commit()
        print("‚úÖ")
        added_cols.append(col_name)
        
    except Exception as e:
        print(f"‚ö†Ô∏è  D√©j√† existe")
        conn.rollback()
        existing_cols.append(col_name)

print(f"\n   ‚úÖ {len(added_cols)} colonnes ajout√©es")
if existing_cols:
    print(f"   ‚ÑπÔ∏è  {len(existing_cols)} colonnes d√©j√† existantes")

# ============================================
# MISE √Ä JOUR DONN√âES
# ============================================

print("\n" + "="*80)
print("üì§ MISE √Ä JOUR DONN√âES")
print("="*80)

# Colonnes √† mettre √† jour (celles qui existent dans pickle)
cols_to_update = [col for col, _ in COLONNES_MANQUANTES if col in df_pickle.columns]

print(f"\n   Colonnes √† remplir: {len(cols_to_update)}")

if cols_to_update:
    batch_size = 500
    total_updated = 0
    
    print(f"\n   Mise √† jour par batch ({batch_size} lignes)...")
    
    for i in tqdm(range(0, len(df_pickle), batch_size), desc="   Progression"):
        batch = df_pickle.iloc[i:i+batch_size]
        
        for _, row in batch.iterrows():
            offre_id = int(row['offre_id'])
            
            # V√©rifier que offre_id existe dans fact_nlp_analysis
            cursor.execute(
                "SELECT 1 FROM fact_nlp_analysis WHERE offre_id = %s",
                (offre_id,)
            )
            
            if not cursor.fetchone():
                continue
            
            # Construire UPDATE dynamique
            set_clauses = []
            values = []
            
            for col in cols_to_update:
                set_clauses.append(f"{col} = %s")
                values.append(row.get(col))
            
            values.append(offre_id)  # Pour WHERE
            
            update_query = f"""
                UPDATE fact_nlp_analysis
                SET {', '.join(set_clauses)}
                WHERE offre_id = %s
            """
            
            cursor.execute(update_query, values)
            total_updated += 1
        
        conn.commit()
    
    print(f"\n   ‚úÖ {total_updated} lignes mises √† jour")

else:
    print("\n   ‚ö†Ô∏è  Aucune donn√©e √† mettre √† jour")

# ============================================
# MISE √Ä JOUR VUE v_offres_nlp_complete
# ============================================

print("\n" + "="*80)
print("üìà MISE √Ä JOUR VUE v_offres_nlp_complete")
print("="*80)

try:
    print("\n   Recr√©ation vue avec nouvelles colonnes...")
    
    cursor.execute("DROP VIEW IF EXISTS v_offres_nlp_complete CASCADE")
    
    cursor.execute("""
    CREATE VIEW v_offres_nlp_complete AS
    SELECT 
        o.offre_id,
        o.job_id_source,
        s.source_name as source,
        o.title,
        e.company_name,
        l.city,
        l.department,
        l.region,
        l.latitude,
        l.longitude,
        c.contract_type,
        c.experience_level,
        o.salary_min,
        o.salary_max,
        (o.salary_min + o.salary_max) / 2 as salary_annual,
        t.date_posted,
        o.description,
        o.url,
        o.scraped_at,
        -- R√©sultats NLP (colonnes originales)
        n.status,
        n.profil_assigned,
        n.score_classification,
        n.competences_found,
        n.topic_id,
        n.cluster_id,
        -- Nouvelles colonnes NLP
        n.num_tokens,
        n.num_competences,
        n.profil_score,
        n.profil_confidence,
        n.profil_second,
        n.profil_second_score,
        n.score_title,
        n.score_description,
        n.score_competences,
        n.cascade_pass,
        n.description_clean,
        n.text_for_sklearn,
        n.tokens,
        n.duration,
        n.salary_text,
        n.source_name
    FROM fact_offres o
    LEFT JOIN dim_source s ON o.source_id = s.source_id
    LEFT JOIN dim_localisation l ON o.localisation_id = l.localisation_id
    LEFT JOIN dim_entreprise e ON o.entreprise_id = e.entreprise_id
    LEFT JOIN dim_contrat c ON o.contrat_id = c.contrat_id
    LEFT JOIN dim_temps t ON o.temps_id = t.temps_id
    LEFT JOIN fact_nlp_analysis n ON o.offre_id = n.offre_id
    """)
    
    conn.commit()
    print("   ‚úÖ Vue recr√©√©e avec toutes colonnes")
    
except Exception as e:
    print(f"\n‚ö†Ô∏è  Erreur vue: {e}")
    conn.rollback()

# ============================================
# V√âRIFICATION FINALE
# ============================================

print("\n" + "="*80)
print("‚úÖ V√âRIFICATION")
print("="*80)

try:
    # Compter colonnes dans vue
    cursor.execute("""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_name = 'v_offres_nlp_complete'
        ORDER BY ordinal_position
    """)
    
    cols_vue = [row[0] for row in cursor.fetchall()]
    
    print(f"\n   üìä Colonnes dans v_offres_nlp_complete: {len(cols_vue)}")
    
    # V√©rifier colonnes ajout√©es pr√©sentes
    missing_in_vue = [col for col, _ in COLONNES_MANQUANTES if col not in cols_vue]
    
    if missing_in_vue:
        print(f"\n   ‚ö†Ô∏è  Colonnes manquantes dans vue: {missing_in_vue}")
    else:
        print(f"\n   ‚úÖ Toutes colonnes pr√©sentes dans vue")
    
    # Test chargement
    cursor.execute("SELECT COUNT(*) FROM v_offres_nlp_complete")
    count = cursor.fetchone()[0]
    print(f"   ‚úÖ {count} offres dans vue")
    
    # Exemples valeurs nouvelles colonnes
    print(f"\n   üìã Exemples valeurs (premi√®re ligne):")
    cursor.execute(f"""
        SELECT {', '.join([col for col, _ in COLONNES_MANQUANTES[:5]])}
        FROM v_offres_nlp_complete 
        WHERE offre_id IS NOT NULL
        LIMIT 1
    """)
    
    result = cursor.fetchone()
    if result:
        for i, (col, _) in enumerate(COLONNES_MANQUANTES[:5]):
            print(f"      {col}: {result[i]}")
    
except Exception as e:
    print(f"\n‚ö†Ô∏è  Erreur v√©rification: {e}")

# ============================================
# FERMETURE
# ============================================

cursor.close()
conn.close()

print("\n" + "="*80)
print("üéâ MIGRATION COLONNES TERMIN√âE !")
print("="*80)

print(f"""
üìä R√âSUM√â:
   - Colonnes ajout√©es: {len(added_cols)}
   - Lignes mises √† jour: {total_updated}
   - Vue recr√©√©e: v_offres_nlp_complete
   
üèóÔ∏è  COLONNES AJOUT√âES:
   ‚úÖ num_tokens, num_competences
   ‚úÖ profil_score, profil_confidence, profil_second
   ‚úÖ score_title, score_description, score_competences
   ‚úÖ cascade_pass
   ‚úÖ description_clean, text_for_sklearn, tokens
   ‚úÖ duration, salary_text, source_name

üîó BASE DE DONN√âES:
   - Host: {DB_CONFIG['host']}
   - Vue: v_offres_nlp_complete (toutes colonnes pickle)
   
üöÄ PROCHAINES √âTAPES:
   1. Tester load_offres_with_nlp() (toutes colonnes dispo)
   2. Relancer application Streamlit
   3. V√©rifier toutes pages fonctionnent
   
‚úÖ Application 100% compatible avec pickle !
""")

print("\n‚úÖ Script termin√© sans erreur !")